{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install <package>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the tweets csv in a Pandas dataframe using ',' as a separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = <enter your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_and_sentiment = df[['text','airline_sentiment']]\n",
    "tweet_and_sentiment.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only positive and negative tweets from the new DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_and_sentiment = <enter your code here>\n",
    "tweet_and_sentiment.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Train and Test datsets 75/25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = <enter your code here>\n",
    "test = <enter your code here>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert sentiment label to numeric category using Pandas factorize method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = <enter your code here>\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The factorize method converts strings into numeric categories and then keeps string categories as an array of index.  \n",
    "so in this `Index(['positive', 'negative'], dtype='object'))` positive = 0 and negative = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_index = labels[1]\n",
    "train['airline_sentiment'] = labels[0]\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start a SageMaker Session and upload the Train Data using the upload_data method.\n",
    "\n",
    "Use key_prefix=prefix+'/training' as a parameter of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sage_maker_session = <enter your code here>\n",
    "prefix = 'tensorflow_sentiment_analysis'\n",
    "training_input_path = sage_maker_session.<enter your code here>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(training_input_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile train.py\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM,Dense\n",
    "from tensorflow.keras.layers import Embedding, Dropout\n",
    "import pandas as pd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=10)\n",
    "    parser.add_argument('--batch-size', type=int, default=100)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.1)\n",
    "    \n",
    "\n",
    "    parser.add_argument('--gpu-count', type=int, default=os.environ['SM_NUM_GPUS'])\n",
    "\n",
    "    # input data and model directories\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "    #parser.add_argument('--test', type=str, default=os.environ['SM_CHANNEL_TEST'])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "    \n",
    "    epochs     = args.epochs\n",
    "    lr         = args.learning_rate\n",
    "    batch_size = args.batch_size\n",
    "    gpu_count  = args.gpu_count\n",
    "    model_dir  = args.model_dir\n",
    "    training_dir   = args.train\n",
    "    \n",
    "    ## get the training data from the CSV file into a Pandas DF, you can use the training_dir argument, use ',' as separator\n",
    "    \n",
    "    training_data = <insert your code here>\n",
    "    tweet = training_data.text.values\n",
    "    labels = training_data.airline_sentiment.values\n",
    "    \n",
    "    num_of_words = 5000\n",
    "    token = Tokenizer(num_words=num_of_words)\n",
    "    token.fit_on_texts(tweet)\n",
    "    \n",
    "    vocab_size = len(token.word_index) + 1 # 1 is added due to 0 index\n",
    "    \n",
    "    tweet_sequence = token.texts_to_sequences(tweet)\n",
    "    \n",
    "    max_len = 200\n",
    "    padded_tweet_sequence = pad_sequences(tweet_sequence, maxlen=max_len)\n",
    "    \n",
    "    # Build the model\n",
    "    embedding_vector_length = 32\n",
    "    ## Import the sequential model https://www.tensorflow.org/guide/keras/sequential_model withput parameter\n",
    "    model = <insert your model here>\n",
    "    \n",
    "    \n",
    "    model.add(Embedding(vocab_size, embedding_vector_length, input_length=max_len) )\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(100)) \n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    ## add a compile argument to the model with\n",
    "    ## loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']\n",
    "    <insert your code here>\n",
    "    \n",
    "    \n",
    "    ## Execute the Model Fit with training data, labels, validation split, epochs, batch_size and level of verbose\n",
    "    <insert your code here>\n",
    "    \n",
    "    ## Save your model using the simple_save --> https://docs.w3cub.com/tensorflow~1.15/saved_model/simple_save\n",
    "    ## use tf.keras.backend.get_session(), os.path.join(model_dir, '1'), {'inputs': model.input}, {t.name: t for t in model.outputs} as params\n",
    "    <use your code here>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_version = tf.__version__\n",
    "tf_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import TemsorFlow from sagemaker package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tensorflow import TensorFlow\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the tf_estimator with the Tensorflow \n",
    "\n",
    "Complete the Parameters\n",
    "https://www.tensorflow.org/api_docs/python/tf/estimator/Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator = TensorFlow(entry_point=, \n",
    "                          role=,\n",
    "                          train_instance_count=1, \n",
    "                          train_instance_type='ml.c5.18xlarge',\n",
    "                          framework_version=tf_version, \n",
    "                          py_version='py3',\n",
    "                          script_mode=True,\n",
    "                          hyperparameters={\n",
    "                              'epochs': 10,\n",
    "                              'batch-size': 64\n",
    "                          }\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_estimator.fit({'train': training_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "endpoint_name = 'tensorflow-sentiment-analysis'+time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "end_point = tf_estimator.deploy(initial_instance_count=1,instance_type='ml.m5.4xlarge',endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(end_point.endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess_texts(text):\n",
    "    \n",
    "    num_of_words = 5000\n",
    "    token = Tokenizer(num_words=num_of_words)\n",
    "    token.fit_on_texts(training_data.text.values)\n",
    "    \n",
    "    tweet_sequence = token.texts_to_sequences(text)\n",
    "    \n",
    "    max_len = 200\n",
    "    padded_tweet_sequence = pad_sequences(tweet_sequence, maxlen=max_len)\n",
    "    \n",
    "    return padded_tweet_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = preprocess_texts(test.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i, tweet in enumerate(test_texts):\n",
    "    labels = ['positive','negative']\n",
    "    print(test.text.values[i])\n",
    "    prediction = end_point.predict(tweet)['predictions']\n",
    "    prediction = np.array(prediction).round().item()\n",
    "    print('Actual sentiment: {} ----- Predicted sentiment  {} \\n'.format(test.airline_sentiment.values[i],labels[int(prediction)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try your own Twwets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_point.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
